# -*- coding: utf-8 -*-
"""Master copy Colored_4microx_fullface

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a1yDyfKAXsXPFsSea30qnpqSH7_09uxb
"""

pip install patool

from google.colab import drive
drive.mount('/content/drive')

"""Runnning for dataset 1"""

#these are the only variables you need to update.
datafile_name = "completed_1.1"
emotionfile_name = "clip_and_emotion_1"
loadModel = "/content/drive/MyDrive/microX/colored_4microx_fullFace_1and2"
saveModel = "/content/drive/MyDrive/microX/colored_4microx_fullFace_1and2"
listofMicroX=["mouth","leftEye","rightEye","nose"]

dataset = "/content/drive/MyDrive/microX/{data}".format(data=datafile_name+".zip")
csvFile = "/content/drive/MyDrive/microX/{label}".format(label=emotionfile_name+".csv")
import patoolib
patoolib.extract_archive(dataset, outdir="/content/Format/")

import os
import shutil
import pandas as pd
classes = pd.read_csv(csvFile)
classes.head()

from glob import glob
listClips = glob("/content/Format/{data}/*".format(data=datafile_name))
print(listClips)


def labeledClips():
  clipList = []
  for i in listClips:
    # i = /content/Format/completed_split/S009_010
    # splitI = [S009_010,S046_004,....]
    splitI = i.split("/")
    clipList.append(splitI[4])
  labeledDF = classes[classes["Clip Name"].isin(clipList)]
  labeledDF['Emotion Code'] =labeledDF['Emotion Code'].astype(int)
  return labeledDF
    

labeledDF = labeledClips()
labeledDF.head()



#split into classes

def reformatFolders(dataframe):
  clipNameList = dataframe["Clip Name"].tolist()
  EmotionCodeList = dataframe["Emotion Code"].tolist()
  for i in range(len(clipNameList)):
    try:
      clipToMove = "/content/Format/{data}/{clip}".format(data=datafile_name, clip=clipNameList[i])
      pathToMove = "Classes/{emotion}/{clip}".format(emotion=EmotionCodeList[i],clip=clipNameList[i])
      shutil.move(clipToMove,pathToMove)
    except: 
      continue

reformatFolders(labeledDF)

#repeat for 1.2

def clipNameEmotion(emotion):
  clipList = []
  pathName = "/content/Classes/{emotion}/*".format(emotion=emotion)
  listClips = glob(pathName)

  for i in listClips:
    splitClips = i.split("/")
    clipList.append(splitClips[4])
  return clipList

totalClip= []
for i in range(8):
  totalClip.append(clipNameEmotion(i))
print(totalClip)

def creating_segments(microx,mode):
  for features in microx:
    for j in range(7):
      emotion=j
      clipList2 = []
      for i in totalClip[emotion]:
        # print(i)
        if(mode==0):
          pathName = "/content/Classes/{emotion}/{clip_no}/{frames}/*".format(emotion=emotion,clip_no = i,frames = i+ "_frame")
        if(mode==1):
          pathName = "/content/Classes/{emotion}/{clip_no}/{frames}/*".format(emotion=emotion,clip_no = i,frames = features)

        listClips = glob(pathName)
        clipList2.append(listClips)
      # print(clipList2)

      #[[],[],[]] so there is 144 videos but more frames inside
      resulting_pathname = []
      for i in range(len(clipList2)):
        frame_no= len(clipList2[i])
        rounded_frame_no = frame_no - (frame_no%10)

        # print(frame_no)
        get_clip_name = clipList2[i][0]
        get_clip_name = get_clip_name.split("/")
        get_clip_name = get_clip_name[4]
        some_list= []
        for j in range(min(rounded_frame_no,40)):
          if(mode==0):
            pathname = "/content/Classes/{emotion}/{clip_no}/{naming}/{frames}".format(emotion=emotion,clip_no = get_clip_name,naming= get_clip_name + "_frame",frames = get_clip_name + "_frame" + str(j) + ".jpg")
          if(mode==1):
            pathname = "/content/Classes/{emotion}/{clip_no}/{naming}/{frames}".format(emotion=emotion,clip_no = get_clip_name,naming= features ,frames = features+ get_clip_name + "_frame" + str(j) + ".jpg")

          some_list.append(pathname)  
        resulting_pathname.append(some_list)
      # print(resulting_pathname)

      segmented_frames = []
      for x in range(len(resulting_pathname)):
        my_list = resulting_pathname[x]
        composite_list = [my_list[x:x+10] for x in range(0, len(my_list),10)]
        segmented_frames.append(composite_list)
      # print(segmented_frames)
      #OUTPUT: [vid1[[batch1],[batch2],[..]],[[],[],[]]]



      clipNo = 0
      frameNo = 0
      for m in segmented_frames:
        for m2 in m:
          if(mode==1):
            path = "Segments/{emotion}/{microx}/{clip}".format(emotion=emotion,clip=clipNo,microx= features)
          if(mode==0):
            path = "Segments/{emotion}/{microx}/{clip}".format(emotion=emotion,clip=clipNo,microx= "fullFace")

          os.makedirs(path, exist_ok=True)
          for m3 in m2:
            clipToMove = m3
            if(mode==0):
              pathToMove = "Segments/{emotion}/{microx}/{clip}/{frame}.jpg".format(emotion=emotion,microx =  "fullFace", clip=clipNo,frame=str(clipNo)+"frame"+str(frameNo))
            if(mode==1):
              pathToMove = "Segments/{emotion}/{microx}/{clip}/{frame}.jpg".format(emotion=emotion,microx = features, clip=clipNo,frame=str(clipNo)+"frame"+str(frameNo))
            frameNo += 1
            # print(clipToMove,pathToMove)
            shutil.move(clipToMove,pathToMove)


          frameNo = 0
          clipNo += 1


creating_segments(listofMicroX,1)
creating_segments(["fullface"],0)



"""Running for dataset 2 (Remember to change the name of Folder for Classesand Format First)"""

#these are the only variables you need to update.
datafile_name = "completed_2.1"
emotionfile_name = "clip_and_emotion_2"

dataset = "/content/drive/MyDrive/microX/{data}".format(data=datafile_name+".zip")
csvFile = "/content/drive/MyDrive/microX/{label}".format(label=emotionfile_name+".csv")
import patoolib
patoolib.extract_archive(dataset, outdir="/content/Format/")

import os
import shutil
import pandas as pd
classes = pd.read_csv(csvFile)
classes.head()

from glob import glob
listClips = glob("/content/Format/{data}/*".format(data=datafile_name))
print(listClips)


def labeledClips():
  clipList = []
  for i in listClips:
    # i = /content/Format/completed_split/S009_010
    # splitI = [S009_010,S046_004,....]
    splitI = i.split("/")
    clipList.append(splitI[4])
  labeledDF = classes[classes["Clip Name"].isin(clipList)]
  labeledDF['Emotion Code'] =labeledDF['Emotion Code'].astype(int)
  return labeledDF
    

labeledDF = labeledClips()
labeledDF.head()

#split into classes

def reformatFolders(dataframe):
  clipNameList = dataframe["Clip Name"].tolist()
  EmotionCodeList = dataframe["Emotion Code"].tolist()
  for i in range(len(clipNameList)):
    try:
      clipToMove = "/content/Format/{data}/{clip}".format(data=datafile_name, clip=clipNameList[i])
      pathToMove = "Classes/{emotion}/{clip}".format(emotion=EmotionCodeList[i],clip=clipNameList[i])
      shutil.move(clipToMove,pathToMove)
    except: 
      continue

reformatFolders(labeledDF)

def clipNameEmotion(emotion):
  clipList = []
  pathName = "/content/Classes/{emotion}/*".format(emotion=emotion)
  listClips = glob(pathName)

  for i in listClips:
    splitClips = i.split("/")
    clipList.append(splitClips[4])
  return clipList

totalClip= []
for i in range(8):
  totalClip.append(clipNameEmotion(i))
print(totalClip)

def creating_segments(microx,mode):
  for features in microx:
    for j in range(7):
      emotion=j
      clipList2 = []
      for i in totalClip[emotion]:
        # print(i)
        if(mode==0):
          pathName = "/content/Classes/{emotion}/{clip_no}/{frames}/*".format(emotion=emotion,clip_no = i,frames = i+ "_frame")
        if(mode==1):
          pathName = "/content/Classes/{emotion}/{clip_no}/{frames}/*".format(emotion=emotion,clip_no = i,frames = features)

        listClips = glob(pathName)
        clipList2.append(listClips)
      # print(clipList2)

      #[[],[],[]] so there is 144 videos but more frames inside
      resulting_pathname = []
      for i in range(len(clipList2)):
        frame_no= len(clipList2[i])
        rounded_frame_no = frame_no - (frame_no%10)

        # print(frame_no)
        get_clip_name = clipList2[i][0]
        get_clip_name = get_clip_name.split("/")
        get_clip_name = get_clip_name[4]
        some_list= []
        for j in range(min(rounded_frame_no,40)):
          if(mode==0):
            pathname = "/content/Classes/{emotion}/{clip_no}/{naming}/{frames}".format(emotion=emotion,clip_no = get_clip_name,naming= get_clip_name + "_frame",frames = get_clip_name + "_frame" + str(j) + ".jpg")
          if(mode==1):
            pathname = "/content/Classes/{emotion}/{clip_no}/{naming}/{frames}".format(emotion=emotion,clip_no = get_clip_name,naming= features ,frames = features+ get_clip_name + "_frame" + str(j) + ".jpg")

          some_list.append(pathname)  
        resulting_pathname.append(some_list)
      # print(resulting_pathname)

      segmented_frames = []
      for x in range(len(resulting_pathname)):
        my_list = resulting_pathname[x]
        composite_list = [my_list[x:x+10] for x in range(0, len(my_list),10)]
        segmented_frames.append(composite_list)
      # print(segmented_frames)
      #OUTPUT: [vid1[[batch1],[batch2],[..]],[[],[],[]]]



      clipNo = 620
      frameNo = 0
      for m in segmented_frames:
        for m2 in m:
          if(mode==1):
            path = "Segments/{emotion}/{microx}/{clip}".format(emotion=emotion,clip=clipNo,microx= features)
          if(mode==0):
            path = "Segments/{emotion}/{microx}/{clip}".format(emotion=emotion,clip=clipNo,microx= "fullFace")

          os.makedirs(path, exist_ok=True)
          for m3 in m2:
            clipToMove = m3
            if(mode==0):
              pathToMove = "Segments/{emotion}/{microx}/{clip}/{frame}.jpg".format(emotion=emotion,microx =  "fullFace", clip=clipNo,frame=str(clipNo)+"frame"+str(frameNo))
            if(mode==1):
              pathToMove = "Segments/{emotion}/{microx}/{clip}/{frame}.jpg".format(emotion=emotion,microx = features, clip=clipNo,frame=str(clipNo)+"frame"+str(frameNo))
            frameNo += 1
            # print(clipToMove,pathToMove)
            shutil.move(clipToMove,pathToMove)


          frameNo = 0
          clipNo += 1


creating_segments(listofMicroX,1)
creating_segments(["fullface"],0)



"""Dataset 3"""









##this is to export segmented data
##replace 1st arg with new name of zip
import shutil
shutil.make_archive('Segments_emotions', 'zip', 'Segments')



def clipNameEmotion(emotion,feature):
  clipList = []
  pathName = "/content/Segments/{emotion}/{microx}/*".format(emotion=emotion,microx=feature)
  print(pathName)
  listClips = glob(pathName)
  print(listClips)
  for i in listClips:
    splitClips = i.split("/")
    clipList.append(splitClips[5])
  return clipList

newTotalClip= []
for e in listofMicroX:
  for i in range(7):
    newTotalClip.append(clipNameEmotion(i,e))
print(newTotalClip)

#get pixel for each frames sequentially FOR MOUTH
#[pixels_frame1,pixels_frame2,...]

#frames_list is pixel of all frames in one vid
#X will be list of framelist for 100(?) vids
import cv2
import numpy as np
img_height , img_width = 60, 60

def unpackPixel(emotion,microExpression,clipName):
  frames_list=[]
  for i in range(10):
    pathName = '/content/Segments/{emotion}/{microx}/{clip}/{filename}.jpg'.format(emotion = emotion, microx=microExpression, clip = clipName  ,  filename = clipName + "frame" + str(i) )
    # frames_list.append(pathName)
    img= cv2.imread(pathName)
    # norm_img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
    if(microExpression=="fullFace"):
      img = cv2.resize(img, (img_height, img_width))

    frames_list.append(img)

  return frames_list

def create_data(listOfClipForEmotion,microx):
    X = []
    Y = []
    counter=0
    classes_list=["0","1","2","3","4","5","6"]      
    for c in classes_list:
        limit=0
        for f in listOfClipForEmotion[int(c)]: #number of clip in each MOUTH classes
           limit +=1
           if (limit<100000):
            frames = unpackPixel(c,microx,f)
            if len(frames) == 10: #where we can split data instead of simply getting 1st 10
              X.append(frames)
              counter += 1
              y = [0]*len(classes_list)
              y[classes_list.index(c)] = 1
              Y.append(y)
    print(counter)
    X = np.asarray(X)
    Y = np.asarray(Y)
    return X,Y
 

X_m,Y_m = create_data(newTotalClip,"mouth")
X_re, Y_re = create_data(newTotalClip,"rightEye")
X_le,Y_le = create_data(newTotalClip,"leftEye")
X_n,Y_n = create_data(newTotalClip,"nose")
# X_rb,Y_rb = create_data(newTotalClip,"rightBrow")
# X_lb,Y_lb = create_data(newTotalClip,"leftBrow")


# to add in main frame
X_main, Y_main = create_data(newTotalClip,"fullFace")

print(X_m.dtype)
print(X_main.dtype)

print(X_m.shape)
print(X_main.shape)

print(Y_m)

from sklearn.model_selection import train_test_split


X_m_train, X_m_test, y_m_train, y_m_test = train_test_split(X_m, Y_m, test_size=0.20, shuffle=True, random_state=0)
X_n_train, X_n_test, y_n_train, y_n_test = train_test_split(X_n, Y_n, test_size=0.20, shuffle=True, random_state=0)
# X_rb_train, X_rb_test, y_rb_train, y_rb_test = train_test_split(X_rb, Y_rb, test_size=0.20, shuffle=True, random_state=0)
# X_lb_train, X_lb_test, y_lb_train, y_lb_test = train_test_split(X_lb, Y_lb, test_size=0.20, shuffle=True, random_state=0)
X_re_train, X_re_test, y_re_train, y_re_test = train_test_split(X_re, Y_re, test_size=0.20, shuffle=True, random_state=0)
X_le_train, X_le_test, y_le_train, y_le_test = train_test_split(X_le, Y_le, test_size=0.20, shuffle=True, random_state=0)
X_main_train, X_main_test, y_main_train, y_main_test = train_test_split(X_main, Y_main, test_size=0.20, shuffle=True, random_state=0)

print(X_m_train.shape)
print(y_m_train.shape)
print(X_n_train.shape)
print(y_n_train.shape)
# print(X_rb_train.shape)
# print(y_rb_train.shape)

print(y_m_train)
print(X_m_train)
print(y_m_train==y_n_train)

import keras
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model 
from keras.layers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping

# model = Sequential()
# model.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last", input_shape = (10, 60, 60, 3)))
# model.add(Dropout(0.2))
# model.add(Flatten())
# model.add(Dense(256, activation="relu"))
# model.add(Dropout(0.3))
# model.add(Dense(7, activation = "softmax"))
 
# model.summary()
 
# opt = keras.optimizers.SGD(lr=0.001)
# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])
 
# earlystop = EarlyStopping(patience=7)
# callbacks = [earlystop]
 
# history = model.fit(x = X_main_train, y = y_main_train, epochs=30, batch_size = 16 , shuffle=True, validation_split=0.2, callbacks=callbacks)



"""Run the next two cells to train the model based on a previously saved model. Indicate the folder where the model is saved in variable "savedModel" in the first few blocks above"""

loadModel = "/content/drive/MyDrive/microX/colored_4microx_fullFace_1and2"
model = keras.models.load_model(loadModel)
model.summary

opt = keras.optimizers.SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])
 
earlystop = EarlyStopping(patience=7)
callbacks = [earlystop]

history = model.fit([X_m_train,X_n_train, X_rb_train, X_lb_train, X_re_train, X_le_train],y_m_train, epochs=30, batch_size = 8 , shuffle=True, validation_split=0.2, callbacks=callbacks)
model.save(saveModel)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""Run below if training model from scratch"""

from keras.models import Input, Model
from keras.layers import Dense

inputA = Input(shape=(10, 60, 60, 3))
a = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputA)
a = Dropout(0.2)(a)
a = Flatten()(a)
a = Dense(256,activation="relu")(a)
a = Dropout(0.3)(a)
a = Dense(7,activation="softmax")(a)
model1 = Model(inputs=inputA ,outputs=a)

inputB = Input(shape=(10, 60, 60, 3))
b = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputB)
b = Dropout(0.2)(b)
b = Flatten()(b)
b = Dense(256,activation="relu")(b)
b = Dropout(0.3)(b)
b = Dense(7,activation="softmax")(b)
model2 = Model(inputs=inputB ,outputs=b)

inputC = Input(shape=(10, 60, 60, 3))
c = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputC)
c = Dropout(0.2)(c)
c = Flatten()(c)
c = Dense(256,activation="relu")(c)
c = Dropout(0.3)(c)
c = Dense(7,activation="softmax")(c)
model3 = Model(inputs=inputC ,outputs=c)

inputD = Input(shape=(10, 60, 60, 3))
d = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputD)
d = Dropout(0.2)(d)
d = Flatten()(d)
d = Dense(256,activation="relu")(d)
d = Dropout(0.3)(d)
d = Dense(7,activation="softmax")(d)
model4 = Model(inputs=inputD ,outputs=d)

inputE = Input(shape=(10, 60, 60, 3))
e = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputE)
e = Dropout(0.2)(e)
e = Flatten()(e)
e = Dense(256,activation="relu")(e)
e = Dropout(0.3)(e)
e = Dense(7,activation="softmax")(e)
model5 = Model(inputs=inputE ,outputs=e)

# inputF = Input(shape=(10, 60, 60, 3))
# f = ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last")(inputF)
# f = Dropout(0.2)(f)
# f = Flatten()(f)
# f = Dense(256,activation="relu")(f)
# f = Dropout(0.3)(f)
# f = Dense(7,activation="softmax")(f)
# model6 = Model(inputs=inputF ,outputs=f)

combined = concatenate([model1.output, model2.output, model3.output, model4.output, model5.output])
# z = Dense(256, activation="relu")(combined)
z = Dense(7, activation="softmax")(combined)
model= Model(inputs=[model1.input, model2.input, model3.input, model4.input, model5.input],outputs=z)
# model = keras.models.load_model(savedModel)

opt = keras.optimizers.SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])
 
earlystop = EarlyStopping(patience=7)
callbacks = [earlystop]
 
history = model.fit([X_m_train,X_n_train, X_re_train, X_le_train, X_main_train],y_m_train, epochs=30, batch_size = 4 , shuffle=True, validation_split=0.2, callbacks=callbacks)
model.save(saveModel)

pred = model.predict([X_m_test,X_n_test, X_re_test, X_le_test, X_main_test])
print(pred)
predicted_output = np.argmax(pred,axis=1)
print(predicted_output)

print(y_m_test)
y_output = np.argmax(y_m_test,axis=1)
print(y_output)

print(saveModel)

model.summary()

#check if data equally distributed
import matplotlib as mpl
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
# y_m['emotion'].value_counts().plot(kind='bar')
# plt.show()
import numpy as np
emotion_counts = Y_m.sum(axis=0)
emotionlabels = ['Netural','Anger','Contempt','Surprise','Fear','Happy','Sad']
ax.bar(emotionlabels,emotion_counts)
# ax.tick_params(axis='x', colors='white')
# ax.tick_params(axis='x', colors='white')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['accuracy', 'val_accuracy'], loc='upper left')
plt.show()


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['loss', 'val_loss'], loc='upper left')
plt.show()

#output array with emotion
def predictedEmotion(possibleEmotion):
  emotionOutput = []
  for i in possibleEmotion:
    likelyEmotion = np.argmax(i)
    emotionOutput.append(likelyEmotion)
  return np.array(emotionOutput)


predictedEmotionArray = predictedEmotion(pred)
print(predictedEmotionArray)

#confusion matrix 
from sklearn.metrics import confusion_matrix
import seaborn as sns
labels = ['Netural','Anger','Contempt','Surprise','Fear','Happy','Sad']
print(predictedEmotion(y_m_test))
cf_matrix = confusion_matrix(predictedEmotion(y_m_test), predictedEmotionArray)
sns.heatmap(cf_matrix/np.sum(cf_matrix), xticklabels=labels, yticklabels=labels, annot=True, 
            fmt='.2%', cmap='Blues')

from sklearn.metrics import multilabel_confusion_matrix

cf_matrix2 = multilabel_confusion_matrix(predictedEmotion(y_m_test), predictedEmotionArray)
print(cf_matrix2)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10,8))
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2,2,2)

# plt.title('Neutral Confusion Matrix',fontsize=5)
ax1.title.set_text('Netural')
df_cm_Neutral = pd.DataFrame(cf_matrix2[0], range(2), range(2))
sns.heatmap(df_cm_Neutral, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax1) # font size

ax2.title.set_text('Anger')
df_cm_Happy = pd.DataFrame(cf_matrix2[1], range(2), range(2))
sns.heatmap(df_cm_Happy, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax2) # font size
plt.show()

fig = plt.figure(figsize=(10,8))
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2,2,2)
ax1.title.set_text('Contempt')
df_cm_Contempt = pd.DataFrame(cf_matrix2[2], range(2), range(2))
sns.heatmap(df_cm_Contempt, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax1) # font size

ax2.title.set_text('Surprise')
df_cm_Surprise = pd.DataFrame(cf_matrix2[3], range(2), range(2))
sns.heatmap(df_cm_Surprise, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax2) # font size

plt.show()

fig = plt.figure(figsize=(10,8))
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2,2,2)
ax1.title.set_text('Fear')
df_cm_fear = pd.DataFrame(cf_matrix2[4], range(2), range(2))
sns.heatmap(df_cm_fear, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax1) # font size

ax2.title.set_text('Happy')
df_cm_happy = pd.DataFrame(cf_matrix2[5], range(2), range(2))
sns.heatmap(df_cm_happy, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax2) # font size

plt.show()

fig = plt.figure(figsize=(10,8))
ax1 = fig.add_subplot(2, 2, 1)
ax1.title.set_text('Sad')
df_cm_sad = pd.DataFrame(cf_matrix2[6], range(2), range(2))
sns.heatmap(df_cm_sad, annot=True, fmt="d", linewidths=.5,cmap="YlGnBu",ax=ax1) # font size

plt.show()

#split data into individual emotions, show the next highest emotion

def groupByEmotion(predictedEmotion,emo):
  eList = []
  for i in range(len(predictedEmotion)):
    if predictedEmotion[i] == emo :
      eList.append(pred[i])
  return eList

def secondHighestEmotion(emotionList):
  secondEmotion = [] 
  for i in emotionList:
    list1 = i.tolist()
    x = sorted(list1)[-2]
    secondEmotion.append(list1.index(x))
  return secondEmotion

def emotionName(codedEmotion):
  newList= []
  labels = ['Netural','Anger','Contempt','Surprise','Fear','Happy','Sad']
  for i in codedEmotion:
    newList.append(labels[i])
  return newList

Neutral = groupByEmotion(predictedEmotionArray,0)
NeutralCorr = emotionName(secondHighestEmotion(Neutral))


anger = groupByEmotion(predictedEmotionArray,1)
angerCorr = emotionName(secondHighestEmotion(anger))



contempt = groupByEmotion(predictedEmotionArray,2)
contemptCorr = emotionName(secondHighestEmotion(contempt))


surprise = groupByEmotion(predictedEmotionArray,3)
surpriseCorr = emotionName(secondHighestEmotion(surprise))



fear = groupByEmotion(predictedEmotionArray,4)
fearCorr = emotionName(secondHighestEmotion(fear))



happy = groupByEmotion(predictedEmotionArray,5)
happyCorr = emotionName(secondHighestEmotion(happy))



sad = groupByEmotion(predictedEmotionArray,6)
sadCorr = emotionName(secondHighestEmotion(sad))

Expressions = ['Netural','Anger','Contempt','Surprise','Fear','Happy','Sad']
fig = plt.figure(figsize=(15,15))

ax = fig.add_subplot(4, 2, 1)
ax1 = fig.add_subplot(4, 2, 2)
ax2 = fig.add_subplot(4, 2, 3)
ax3 = fig.add_subplot(4, 2, 4)
ax4 = fig.add_subplot(4, 2, 5)
ax5 = fig.add_subplot(4,2,6)
ax6 = fig.add_subplot(4,2,7)

def color(corr):
  colorList = []
  colors = ['#ea2c62','#adb36e','#394867','#f4abc4','#1f6f8b','#595b83','#fad5ad']
  for i in corr:
    colorList.append(colors[i])
  return colorList

ax.scatter(range(0, len(NeutralCorr)),NeutralCorr,c = color(secondHighestEmotion(Neutral)) )
ax1.scatter(range(0, len(angerCorr)),angerCorr,c = color(secondHighestEmotion(anger)) )
ax2.scatter(range(0, len(contemptCorr)),contemptCorr,c = color(secondHighestEmotion(contempt)) )
ax3.scatter(range(0, len(surpriseCorr)),surpriseCorr,c = color(secondHighestEmotion(surprise)) )
ax4.scatter(range(0, len(fearCorr)),fearCorr,c = color(secondHighestEmotion(fear)) )
ax5.scatter(range(0, len(happyCorr)),happyCorr,c = color(secondHighestEmotion(happy)) )
ax6.scatter(range(0, len(happyCorr)),happyCorr,c = color(secondHighestEmotion(happy)) )


ax.title.set_text('Netural')
ax1.title.set_text('Anger')
ax2.title.set_text('Contempt')
ax3.title.set_text('Surprise')
ax4.title.set_text('Fear')
ax5.title.set_text('Happy')
ax6.title.set_text('Sad')


plt.show()

#recall,precision,F1 score

from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(predictedEmotion(y_m_test), predictedEmotionArray)

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

x = np.array([0,1,2,3,4,5,6])

my_xticks = ['Netural','Anger','Contempt','Surprise','Fear','Happy','Sad']
plt.xticks(x, my_xticks)

plt.plot(precision)
plt.plot(recall)
plt.plot(fscore)

plt.title('Precision, Recall, fscore')
plt.legend(['Precision', 'Recall' , 'fscore'], loc='upper right')

plt.show()





















