# -*- coding: utf-8 -*-
"""cds project - LSTM with action videos

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AU0lHVT948GFdr_7QmIuNMk_-pCOecZr
"""

pip install keras-video-generators

import os
import glob
import keras
# use sub directories names as classes
classes = [i.split(os.path.sep)[1] for i in glob.glob('hmdb51_org/*')]
classes.sort()
# some global par
print(classes)

import rarfile
rf = rarfile.RarFile('hmdb51_org.rar')
for f in rf.infolist():
    print(f.filename, f.file_size)
    openf=rf.open(f.filename)

os.getcwd()

from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir() if isfile(join('',f))]
HighJump = onlyfiles[1:]

HighJump

pip install patool

import patoolib
patoolib.extract_archive("hmdb51_org.rar", outdir="/content/videos/")

os.chdir(path = "/content/videos/")
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir() if isfile(join('',f))]
classes = onlyfiles[1:]

print(len(classes))
print(classes)

# Commented out IPython magic to ensure Python compatibility.
import cv2     # for capturing videos
import math   # for mathematical operations
import matplotlib.pyplot as plt    # for plotting the images
# %matplotlib inline
import pandas as pd
from keras.preprocessing import image   # for preprocessing the images
import numpy as np    # for mathematical operations
from keras.utils import np_utils
from skimage.transform import resize   # for resizing images
from sklearn.model_selection import train_test_split
from glob import glob
from tqdm import tqdm

# extract 3 classes clap,climb,eat
patoolib.extract_archive("/content/videos/clap.rar", outdir="/content/classes")

patoolib.extract_archive("/content/videos/climb.rar", outdir="/content/classes")

patoolib.extract_archive("/content/videos/eat.rar", outdir="/content/classes")

classes_list = os.listdir("/content/classes")
print(classes_list)

## testing for one video


vidObj = cv2.VideoCapture("/content/classes/clap/A_Round_of_Applause_clap_u_cm_np1_fr_med_1.avi")
frames_list = []

count = 1

while count <= 70: 
        
    success, image = vidObj.read() 
    if success:
        image = cv2.resize(image, (img_height, img_width))
        frames_list.append(image)
        count += 1
    else:
        print("Defected frame")
        break

frames_list

import keras
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model 
from keras.layers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
 
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
 
 
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import multilabel_confusion_matrix
 
data_dir = "/content/classes"
img_height , img_width = 64, 64
seq_len = 70
 
classes = ["clap", "climb", "eat"]
 
#  Creating frames from videos
 
def frames_extraction(video_path):
    frames_list = []
     
    vidObj = cv2.VideoCapture(video_path)
    count = 1
 
    while count <= seq_len: 
         
        success, image = vidObj.read() 
        if success:
            image = cv2.resize(image, (img_height, img_width))
            frames_list.append(image)
            count += 1
        else:
            print("Defected frame")
            break
 
            
    return frames_list
 
def create_data(input_dir):
    X = []
    Y = []
     
    classes_list = os.listdir(input_dir)
     
    for c in classes_list:
        print(c)
        files_list = os.listdir(os.path.join(input_dir, c))
        for f in files_list:
           frames = frames_extraction(os.path.join(os.path.join(input_dir, c), f))
           if len(frames) == seq_len:
            X.append(frames)

            y = [0]*len(classes)
            y[classes.index(c)] = 1
            Y.append(y)
    X = np.asarray(X)
    Y = np.asarray(Y)
    return X, Y
 
X, Y = create_data(data_dir)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, shuffle=True, random_state=0)

print(X_train.shape)
print(y_train.shape)

model = Sequential()
model.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last", input_shape = (seq_len, img_height, img_width, 3)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(256, activation="relu"))
model.add(Dropout(0.3))
model.add(Dense(3, activation = "softmax"))
 
model.summary()
 
opt = keras.optimizers.SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy"])
 
earlystop = EarlyStopping(patience=7)
callbacks = [earlystop]
 
history = model.fit(x = X_train, y = y_train, epochs=20, batch_size = 8 , shuffle=True, validation_split=0.2, callbacks=callbacks)

pred =model.predict(X_test)
y_pred = np.argmax(pred,axis=1)

y_test = np.argmax(y_test,axis=1)

print(y_test)

print(y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

